{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Classification to Classify Walking and Jogging Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "D:\\Anaconda\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#importing all the required libraries and have imported keras to use on the theano backend\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution3D, MaxPooling3D\n",
    "\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "import theano\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  I considered a variable at first to store the entire training set and used a 3D convolutional Neural Network to try and classify the videos by first extracting the features in terms of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows,img_cols,img_depth=16,16,15\n",
    "X_tr=[]           # variable to store entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code is written to read the jogging action class videos and capturing its corresponding frames from the videos and from the output we see we are able to store 25 frames per second. Frames are extracted per second as each video is of different time duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n"
     ]
    }
   ],
   "source": [
    "#Reading the jogging action class\n",
    "jogging_videos = os.listdir('D://Video Classification Assignment//Data//Jogging//')\n",
    "\n",
    "for videos in jogging_videos:\n",
    "    videos = 'D://Video Classification Assignment//Data//Jogging//' + videos\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videos)\n",
    "    fps = cap.get(5)\n",
    "    print(\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
    "\n",
    "    for k in range(15):\n",
    "        ret, frame = cap.read()\n",
    "        frame=cv2.resize(frame,(img_rows,img_cols),interpolation=cv2.INTER_AREA)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames.append(gray)\n",
    "\n",
    "#We can uncomment the lines below to display the required frames that we have extracted. But at\n",
    "#certain point there are too many frames to display and will mostly hang. So uncomment these\n",
    "#lines only if you are sure you want to see the images of every frame from every video\n",
    "\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        #plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    input=np.array(frames)\n",
    "\n",
    "    print (input.shape)\n",
    "    ipt=np.rollaxis(np.rollaxis(input,2,0),2,0)\n",
    "    print (ipt.shape)\n",
    "\n",
    "    X_tr.append(ipt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above output we can see we have been able to extract 25 frames per second.\n",
    "\n",
    "#### The below code is written to read the walking action class videos and capturing its corresponding frames from the videos and from the output we see we are able to store 25 frames per second. Frames are extracted per second as each video is of different time duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n",
      "Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): 25.0\n",
      "(15, 16, 16)\n",
      "(16, 16, 15)\n"
     ]
    }
   ],
   "source": [
    "#Reading the Walking action class\n",
    "walking_videos = os.listdir('D://Video Classification Assignment//Data//Walking//')\n",
    "\n",
    "for videos_walking in walking_videos:\n",
    "    videos_walking = 'D://Video Classification Assignment//Data//Walking//' + videos_walking\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videos_walking)\n",
    "    fps = cap.get(5)\n",
    "    print(\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
    "\n",
    "    for k in range(15):\n",
    "        ret, frame = cap.read()\n",
    "        frame=cv2.resize(frame,(img_rows,img_cols),interpolation=cv2.INTER_AREA)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames.append(gray)\n",
    "\n",
    "        #plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "        #plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "        #plt.show()\n",
    "        #cv2.imshow('frame',gray)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    input=np.array(frames)\n",
    "\n",
    "    print (input.shape)\n",
    "    ipt=np.rollaxis(np.rollaxis(input,2,0),2,0)\n",
    "    print (ipt.shape)\n",
    "\n",
    "    X_tr.append(ipt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "X_tr_array = np.array(X_tr)   # convert the frames read and store it into a numpy array.\n",
    "\n",
    "num_samples = len(X_tr_array)\n",
    "print (num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train shape: (104, 16, 16, 15)\n",
      "(104, 1, 16, 16, 15) train samples\n"
     ]
    }
   ],
   "source": [
    "#Assign Label to each class\n",
    "label=np.ones((num_samples,),dtype = int)\n",
    "label[0:52]= 0#setting label as 0 for Jogging videos\n",
    "label[52:104] = 1##setting the lab\n",
    "\n",
    "\n",
    "train_data = [X_tr_array,label]\n",
    "\n",
    "(X_train, y_train) = (train_data[0],train_data[1])\n",
    "print('X_Train shape:', X_train.shape)\n",
    "\n",
    "train_set = np.zeros((num_samples, 1, img_rows,img_cols,img_depth))\n",
    "\n",
    "for h in range(num_samples):\n",
    "    train_set[h][0][:][:][:]=X_train[h,:,:,:]\n",
    " \n",
    "\n",
    "image_depth_size = 15    # img_depth or number of frames used for each video\n",
    "\n",
    "print(train_set.shape, 'train samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN training parameters\n",
    "batch_size = 2\n",
    "nb_classes = 2#The 2 classes of Jogging and Walking.\n",
    "nb_epoch =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "\n",
    "# number of convolutional filters to use at each layer\n",
    "nb_filters = [32, 32]##using 32 filters for both the first and 2nd layer\n",
    "\n",
    "# level of pooling to perform at each layer (POOL x POOL)\n",
    "nb_pool = [3, 3]#Using 3X3 pooling for each layer\n",
    "\n",
    "# level of convolution to perform at each layer (CONV x CONV)\n",
    "nb_conv = [5,5]#applying 5x5 convolution in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code below is done for preprocessing and scaling. Training is set to be of float type from the numpy arrays.\n",
    "#### We substract the mean and perform max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.astype('float32')\n",
    "\n",
    "train_set -= np.mean(train_set)\n",
    "\n",
    "train_set /=np.max(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part of the code is done for the removal of negative dimensions and hence ordering of the dimensions are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we define the model using Keras Conv3D architecutures and using a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(32, (5, 5, 5), input_shape=(1, 16, 16..., activation=\"relu\")`\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  if sys.path[0] == '':\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, kernel_initializer=\"normal\")`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution3D(nb_filters[0],kernel_dim1=nb_conv[0], kernel_dim2=nb_conv[0], kernel_dim3=nb_conv[0], input_shape=(1, img_rows, img_cols, image_depth_size), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(nb_pool[0], nb_pool[0], nb_pool[0])))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, init='normal', activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes,init='normal'))\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#We are using categorical crossentropy since we are doing a classification task, we could\n",
    "#also use binary_crossentropy also.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data preferably an 80:20 split. 80% for training and 20% for validation and testing.\n",
    "#### This is done using Sklearn's train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, X_val_new, y_train_new,y_val_new =  train_test_split(train_set, Y_train, test_size=0.2, random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 83 samples, validate on 21 samples\n",
      "Epoch 1/50\n",
      "83/83 [==============================] - 13s 159ms/step - loss: 0.7814 - val_loss: 0.8220\n",
      "Epoch 2/50\n",
      "83/83 [==============================] - 12s 147ms/step - loss: 0.7730 - val_loss: 0.6997\n",
      "Epoch 3/50\n",
      "83/83 [==============================] - 12s 139ms/step - loss: 0.7105 - val_loss: 0.6391\n",
      "Epoch 4/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.7525 - val_loss: 0.6215\n",
      "Epoch 5/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.7089 - val_loss: 0.6528\n",
      "Epoch 6/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.7249 - val_loss: 0.6646\n",
      "Epoch 7/50\n",
      "83/83 [==============================] - 11s 131ms/step - loss: 0.6771 - val_loss: 0.6805\n",
      "Epoch 8/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.6574 - val_loss: 0.9044\n",
      "Epoch 9/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.6885 - val_loss: 0.5624\n",
      "Epoch 10/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.6992 - val_loss: 0.6016\n",
      "Epoch 11/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.6701 - val_loss: 0.5587\n",
      "Epoch 12/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.7321 - val_loss: 0.5079\n",
      "Epoch 13/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.7728 - val_loss: 0.6531\n",
      "Epoch 14/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.6766 - val_loss: 0.5365\n",
      "Epoch 15/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.6575 - val_loss: 0.6360\n",
      "Epoch 16/50\n",
      "83/83 [==============================] - 11s 130ms/step - loss: 0.6550 - val_loss: 0.6871\n",
      "Epoch 17/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.6907 - val_loss: 0.5442\n",
      "Epoch 18/50\n",
      "83/83 [==============================] - 12s 142ms/step - loss: 0.7084 - val_loss: 0.5533\n",
      "Epoch 19/50\n",
      "83/83 [==============================] - 10s 125ms/step - loss: 0.6529 - val_loss: 0.5121\n",
      "Epoch 20/50\n",
      "83/83 [==============================] - 11s 135ms/step - loss: 0.6406 - val_loss: 0.5303\n",
      "Epoch 21/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.6254 - val_loss: 0.6501\n",
      "Epoch 22/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.6442 - val_loss: 0.5702\n",
      "Epoch 23/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.6599 - val_loss: 0.5233\n",
      "Epoch 24/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.6739 - val_loss: 0.7023\n",
      "Epoch 25/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.5607 - val_loss: 1.2447\n",
      "Epoch 26/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.6302 - val_loss: 0.6929\n",
      "Epoch 27/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.6416 - val_loss: 0.5443\n",
      "Epoch 28/50\n",
      "83/83 [==============================] - 12s 146ms/step - loss: 0.7256 - val_loss: 0.5080\n",
      "Epoch 29/50\n",
      "83/83 [==============================] - 12s 145ms/step - loss: 0.5927 - val_loss: 0.5733\n",
      "Epoch 30/50\n",
      "83/83 [==============================] - 11s 135ms/step - loss: 0.7563 - val_loss: 0.6657\n",
      "Epoch 31/50\n",
      "83/83 [==============================] - 11s 131ms/step - loss: 0.5683 - val_loss: 0.5430\n",
      "Epoch 32/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.5689 - val_loss: 0.5199\n",
      "Epoch 33/50\n",
      "83/83 [==============================] - 11s 131ms/step - loss: 0.5500 - val_loss: 0.7523\n",
      "Epoch 34/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.6796 - val_loss: 0.9025\n",
      "Epoch 35/50\n",
      "83/83 [==============================] - 11s 131ms/step - loss: 0.6752 - val_loss: 0.5677\n",
      "Epoch 36/50\n",
      "83/83 [==============================] - 12s 141ms/step - loss: 0.5335 - val_loss: 0.5609\n",
      "Epoch 37/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.5549 - val_loss: 0.7562\n",
      "Epoch 38/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.5524 - val_loss: 0.6996\n",
      "Epoch 39/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.6815 - val_loss: 0.5606\n",
      "Epoch 40/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.5797 - val_loss: 0.6849\n",
      "Epoch 41/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.5207 - val_loss: 0.8451\n",
      "Epoch 42/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.7032 - val_loss: 0.6619\n",
      "Epoch 43/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.5507 - val_loss: 0.6432\n",
      "Epoch 44/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.6546 - val_loss: 0.6151\n",
      "Epoch 45/50\n",
      "83/83 [==============================] - 11s 132ms/step - loss: 0.6165 - val_loss: 0.6527\n",
      "Epoch 46/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.5323 - val_loss: 0.8293\n",
      "Epoch 47/50\n",
      "83/83 [==============================] - 11s 134ms/step - loss: 0.5244 - val_loss: 0.8069\n",
      "Epoch 48/50\n",
      "83/83 [==============================] - 11s 133ms/step - loss: 0.6398 - val_loss: 0.6346\n",
      "Epoch 49/50\n",
      "83/83 [==============================] - 11s 135ms/step - loss: 0.5213 - val_loss: 0.5990\n",
      "Epoch 50/50\n",
      "83/83 [==============================] - 11s 137ms/step - loss: 0.5696 - val_loss: 0.6967\n"
     ]
    }
   ],
   "source": [
    "training_model = model.fit(X_train_new, y_train_new, validation_data=(X_val_new,y_val_new),\n",
    "          batch_size=batch_size,nb_epoch = nb_epoch,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are using the Keras model evaluation to print the error metrics and displaying the test score and \n",
    "#### accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 8ms/step\n",
      "Test score: 0.6920240862028939\n",
      "Test accuracy: 0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_val_new, y_val_new, batch_size=batch_size)\n",
    "print(\"Test score:\", score)\n",
    "print(\"Test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
